{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM-SVM Metaclassifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPm4ZJ4Pombp"
      },
      "source": [
        "Since SVM's gave the best results in our case we used a metaclassifier approach. Wherein we performed our experiments in 2 stages. The first stage consisted of a normal SVM classifier where the features were the Tf-Idf feature vectors. For the next stage we stored the output probabilities and along with the counts of placenames and geographic feature types extracted using my gazetteer lookup module and ran the classifier again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_7o1AFbkNze"
      },
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re \n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics , model_selection\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IozvKT_vovyi"
      },
      "source": [
        "rows=[]\n",
        "filename=('18828_dataset_exp3_metaclassifier.csv')\n",
        "with open(filename, 'r') as csvfile: \n",
        "    data = csv.reader(csvfile) \n",
        "    for row in data:\n",
        "        rows.append(row)\n",
        "print(len(rows))\n",
        "\n",
        "print(len(rows))\n",
        "for i in range(10):\n",
        "    print(rows[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKVXMjV6o56y"
      },
      "source": [
        "def remove(list): \n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in list] \n",
        "    return list\n",
        "\n",
        "rows=small_dataset\n",
        "temp=[]\n",
        "sentence_list=[]\n",
        "cnt_placenames=[]\n",
        "cnt_geofeatures=[]\n",
        "Y=[]\n",
        "X=[]\n",
        "    \n",
        "n=len(rows)\n",
        "for i in range(n):\n",
        "    words=[]\n",
        "    l=[]\n",
        "    l=rows[i][0].split(' ')\n",
        "    l=remove(l)\n",
        "    Y.append(rows[i][2])\n",
        "    #seperating the counts to be used in the second phase.\n",
        "    cnt_placenames.append(rows[i][4])\n",
        "    cnt_geofeatures.append(rows[i][3])\n",
        "    k=l.index(rows[i][1])\n",
        "   \n",
        "    l_limit=k-5\n",
        "    u_limit=k+5\n",
        "    if l_limit<=0:\n",
        "        l_limit=0\n",
        "\n",
        "    if u_limit>len(l):\n",
        "        u_limit=len(l)\n",
        "\n",
        "    words=l[l_limit:u_limit+1]\n",
        "    words.remove(l[k])\n",
        "\n",
        "    sw = stopwords.words('english')\n",
        "    for i in words:\n",
        "        for j in sw:\n",
        "            if i == j:\n",
        "                words.remove(i)\n",
        "\n",
        "    new_words=[]\n",
        "    for i in words:\n",
        "        stemmer=SnowballStemmer('english')\n",
        "        k=stemmer.stem(i)\n",
        "        new_words.append(k)\n",
        "\n",
        "    final_string=''\n",
        "    for i in new_words:\n",
        "        final_string=final_string+' '+i\n",
        "    sentence_list.append(final_string)\n",
        "    # f=open('/Users/abhibhagupta/Downloads/newbig.txt','a+')\n",
        "    # f.write(final_string+'\\n')\n",
        "\n",
        "print(len(sentence_list))\n",
        "print(len(Y))\n",
        "print(len(cnt_geofeatures))\n",
        "print(len(cnt_placenames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4CBQ1OApQDU"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "matrix=TfidfVectorizer(max_features=1000)\n",
        "X = matrix.fit_transform(sentence_list).toarray()\n",
        "\n",
        "X_data=np.array(X)\n",
        "Y_data=np.array(Y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYvIghfvpcej"
      },
      "source": [
        "We performed hold out validation to get the probability values for the whole dataset by appending them to a list at each iteration.We are basically training on 90% of the dataset and predicting on the left over 10% dataset. The obtained probability values will be used as features in the second phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TabX-FXApX02"
      },
      "source": [
        "\n",
        "i=0\n",
        "j=1883\n",
        "itr=1\n",
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='rbf', gamma=0.01, C=10, probability=True) \n",
        "final_y=[]\n",
        "new_dataset=[]\n",
        "while i<=16939 and j<=18822:\n",
        "    print('iteration',itr)\n",
        "    \n",
        "    X_train=[]\n",
        "    X_test=[]\n",
        "    y_train=[]\n",
        "    y_test=[]\n",
        "    f_geofeatures=[]\n",
        "    f_placenames=[]\n",
        "\n",
        "    for a in range(0,i):\n",
        "        X_train.append(X[a])\n",
        "        y_train.append(Y[a])\n",
        "    \n",
        "    for a in range(j,18822):\n",
        "        X_train.append(X[a])\n",
        "        y_train.append(Y[a])\n",
        "\n",
        "    for b in range(i,j):\n",
        "        X_test.append(X[b])\n",
        "        y_test.append(Y[b])\n",
        "        f_geofeatures.append(cnt_geofeatures[b])\n",
        "        f_placenames.append(cnt_placenames[b])\n",
        "        final_y.append(Y[b])\n",
        "\n",
        "    X_train=np.array(X_train)\n",
        "    y_train=np.array(y_train)\n",
        "    X_test=np.array(X_test)\n",
        "    y_test=np.array(y_test)\n",
        "\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict_log_proba(X_test)\n",
        "    y_pred=y_pred.tolist()\n",
        "    class_prob=[]\n",
        "    for p in y_pred:\n",
        "        class_prob.append(max(p[1],p[0]))\n",
        "    final_feature_arr=[]\n",
        "    for q in range(0,1883):\n",
        "        final_feature_arr.append([class_prob[q],f_geofeatures[q],f_placenames[q]])\n",
        "    new_dataset.append(final_feature_arr)\n",
        "    i=i+1883\n",
        "    j=j+1883\n",
        "    itr=itr+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1TELlGqNZI"
      },
      "source": [
        "final_feature_dataset=[]\n",
        "for i in new_dataset:\n",
        "    for j in i:\n",
        "        final_feature_dataset.append(j)\n",
        "final_feature_dataset= np.array(final_feature_dataset)\n",
        "import sklearn\n",
        "final_X = sklearn.preprocessing.normalize(final_feature_dataset, norm='l2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kwFj7hiqbL0"
      },
      "source": [
        "# final train-test split\n",
        "X_train_new=[]\n",
        "X_test_new=[]\n",
        "y_train_new=[]\n",
        "y_test_new=[]\n",
        "\n",
        "for l in range(0,15252):\n",
        "    X_train_new.append(final_X[l])\n",
        "    y_train_new.append(final_y[l])\n",
        "\n",
        "for m in range(15252,16947):\n",
        "    X_test_new.append(final_X[m])\n",
        "    y_test_new.append(final_y[m])\n",
        "\n",
        "\n",
        "X_train_new=np.array(X_train_new)\n",
        "y_train_new=np.array(y_train_new)\n",
        "X_test_new=np.array(X_test_new)\n",
        "y_test_new=np.array(y_test_new)\n",
        "\n",
        "\n",
        "print(len(X_train_new))\n",
        "print(len(y_train_new))\n",
        "print(len(X_test_new))\n",
        "print(len(y_test_new))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNzWQVZmqjH3"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC(kernel='rbf', gamma=0.01, C=10, probability=True) \n",
        "clf.fit(X_train_new, y_train_new)\n",
        "\n",
        "y_pred_new = clf.predict(X_test_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlvIDXveqsDo"
      },
      "source": [
        "\n",
        "conf_mat = [[0,0],[0,0]]\n",
        "y_pred=y_pred_new.tolist()\n",
        "y_test=y_test_new.tolist()\n",
        "print(\"ypred \")\n",
        "print(y_pred)\n",
        "print(\"length of ypred compressed\")\n",
        "print(len(y_pred))\n",
        "print(\"ytest compressed\")\n",
        "print(y_test)\n",
        "print(len(y_test))\n",
        "print(\"\\n\")\n",
        "i=0\n",
        "print(\"confusion matrix:\")\n",
        "\n",
        "while i < len(y_pred):\n",
        "    \n",
        "    if y_pred[i]=='1' and y_test[i]=='1': \n",
        "        conf_mat[1][1] +=1\n",
        "    elif y_pred[i]=='0' and  y_test[i]=='1': \n",
        "        conf_mat[0][1] +=1\n",
        "    elif y_pred[i]=='1' and  y_test[i]=='0': \n",
        "        conf_mat[1][0] +=1\n",
        "    elif y_pred[i]=='0' and y_test[i]== '0': \n",
        "        conf_mat[0][0] +=1\n",
        "    i=i+1\n",
        "print(conf_mat)\n",
        "for list1 in conf_mat:\n",
        "\tprint (list1)\n",
        "\n",
        "print(\"\\n\")\t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGWa4a4PqxXK"
      },
      "source": [
        "print(\"\\n\")\t\n",
        "r = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[1][0]+1))\n",
        "p = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[0][1]+1))\n",
        "\n",
        "acc= (float(conf_mat[0][0])+float(conf_mat[1][1]))/(float(conf_mat[0][0])+float(conf_mat[0][1])+float(conf_mat[1][0])+float(conf_mat[1][1]))\n",
        "F1=2*p*r/(p+r)\n",
        "#precision recall F1 scores and accuracy\n",
        "print(str(p)+' '+str(r)+' '+str(F1)+' '+str(acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}