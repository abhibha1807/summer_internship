{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeoSpatial_ULMFIT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pWcweo5QQ1R2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fVCOFc5g7S"
      },
      "source": [
        "learner = load_learner('/content/',test=data_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE47FlVUU7Tb"
      },
      "source": [
        "(learn_c.data.__class__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or_UNMp7NXBj"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDfSwQSgasRM",
        "outputId": "96d96064-c9a6-4a5d-99b9-ca582bcbe424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/wordemb_dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instance</th>\n",
              "      <th>prep</th>\n",
              "      <th>geo-feature</th>\n",
              "      <th>placename</th>\n",
              "      <th>sum</th>\n",
              "      <th>or</th>\n",
              "      <th>label exp1</th>\n",
              "      <th>label exp 2</th>\n",
              "      <th>label exp 3</th>\n",
              "      <th>label exp 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>any project searching for a shared female ide...</td>\n",
              "      <td>for</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>searching a shared female identity in the wor...</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>misleading yet the shared endeavour by women ...</td>\n",
              "      <td>by</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shared endeavour women to succeed in their ch...</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>succeed their chosen field and on their own t...</td>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            instance  ... label exp 4\n",
              "0   any project searching for a shared female ide...  ...         0.0\n",
              "1   searching a shared female identity in the wor...  ...         0.0\n",
              "2   misleading yet the shared endeavour by women ...  ...         0.0\n",
              "3   shared endeavour women to succeed in their ch...  ...         0.0\n",
              "4   succeed their chosen field and on their own t...  ...         0.0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvolP6QZeLgH"
      },
      "source": [
        "test_df=df[:1882].copy()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqETUs8VendI"
      },
      "source": [
        "np.set_printoptions(precision=4, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-sKppT5ewuz"
      },
      "source": [
        "!wget http://files.fast.ai/models/glove_50_glove_100.tgz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wh9Lu-7e2e7"
      },
      "source": [
        "!tar xvzf glove_50_glove_100.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELXyIgpifD4s"
      },
      "source": [
        "vecs = np.load(\"glove_vectors_100d.npy\")\n",
        "vecs50 = np.load(\"glove_vectors_50d.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLm4yFvQg5m9"
      },
      "source": [
        "with open('words.txt') as f:\n",
        "    content = f.readlines()\n",
        "words = [x.strip() for x in content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWWhb0-6g6rd"
      },
      "source": [
        "import json\n",
        "wordidx = json.load(open('wordsidx.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOveD7zHCTad"
      },
      "source": [
        "# for i in list(df['instance']):\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IS3YcdQg7Bn"
      },
      "source": [
        "\n",
        "#X = np.array([wordidx[word] for word in list(df['instance']))\n",
        "X=[]\n",
        "for i in (list(df['instance'])):\n",
        "  # print(i)\n",
        "  sentence_vector=[]\n",
        "  try:\n",
        "    for word in i.split(' '):\n",
        "      if word == '':\n",
        "        continue\n",
        "      #print(word)\n",
        "      try:\n",
        "        sentence_vector.append(wordidx[word])\n",
        "      except:\n",
        "        pass\n",
        "  except:\n",
        "    pass\n",
        "  X.append((sentence_vector))\n",
        "# X=np.array(X)\n",
        "# X.shape\n",
        "type(X),len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UltVMlq0g7FL"
      },
      "source": [
        "(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6x0mgpVfMqO"
      },
      "source": [
        "words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPfCPBx1geqB"
      },
      "source": [
        "#wordidx[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JNGD1UDGYMr"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test=train_test_split(X,(list(df['label exp 3'])),test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE6YMYnBQy6Q"
      },
      "source": [
        "type(X_test),type(X_train),type(y_train),type(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs4eL84zK8UI"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDOs1vFQPcA7"
      },
      "source": [
        "', '.join(map(str, X_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6iFS0xOPe79"
      },
      "source": [
        "idx=wordidx\n",
        "idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJwy0ZRrPwsr"
      },
      "source": [
        "idx2word = {v: k for k, v in idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIkdsctcP3m-"
      },
      "source": [
        "idx2word[8206]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LxtcebcQIdo"
      },
      "source": [
        "' '.join([idx2word[o] for o in X_train[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYTsOEhFQgsD"
      },
      "source": [
        "type(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtekaWg5L5x_"
      },
      "source": [
        "vocab_size = 5000\n",
        "# l1=[]\n",
        "# for s in X_train:\n",
        "#   for i in s:\n",
        "#     if i<vocab_size-1:\n",
        "#       # print('ping')\n",
        "#       l1.append(i)\n",
        "#     else:\n",
        "#       # print('ping')\n",
        "#       l1.append(vocab_size-1)\n",
        "# # trn\n",
        "\n",
        "# l2=[]\n",
        "# for p in X_test:\n",
        "#   for j in p:\n",
        "#     if j<vocab_size-1:\n",
        "#       # print('ping')\n",
        "#       l2.append(j)\n",
        "#     else:\n",
        "#       # print('ping')\n",
        "#       l2.append(vocab_size-1)\n",
        "\n",
        "# trn=[np.array(l1)]\n",
        "# test=[np.array(l2)]\n",
        "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in (X_train)]\n",
        "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in (X_test)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjD8EcDQM1ps"
      },
      "source": [
        "lens = np.array([len(instance) for instance in trn])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCeWsVaEM2I_"
      },
      "source": [
        "(lens.max(), lens.min(), lens.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw7h-V-KLGMn"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "seq_len = 50\n",
        "try:\n",
        "  trn = sequence.pad_sequences(X_train, maxlen=seq_len, value=0)\n",
        "  test = sequence.pad_sequences(X_test, maxlen=seq_len, value=0)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgtVFzPqLhY7"
      },
      "source": [
        "len(trn),trn[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKiytzzhLsnv"
      },
      "source": [
        "trn.shape,test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXAer7SaNcQY"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkWQmsOeNeiV"
      },
      "source": [
        "conv1 = Sequential([\n",
        "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.4),\n",
        "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    MaxPooling1D(),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_lYGlrsNhiA"
      },
      "source": [
        "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_7Z_ixCNkpe"
      },
      "source": [
        "conv1.fit(trn, y_train, validation_data=(test, y_test), nb_epoch=4, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGAzj-suNmXl"
      },
      "source": [
        "conv1.optimizer.lr=1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vog0TD08DJJ-"
      },
      "source": [
        "conv1.fit(trn, y_train, validation_data=(test, y_test), nb_epoch=4, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_NDfmk_DLBg"
      },
      "source": [
        "def create_emb():\n",
        "    n_fact = vecs50.shape[1]\n",
        "    emb = np.zeros((vocab_size, n_fact))\n",
        "\n",
        "    for i in range(1,len(emb)):\n",
        "        word = idx2word[i]\n",
        "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
        "            src_idx = wordidx[word]\n",
        "            emb[i] = vecs50[src_idx]\n",
        "        else:\n",
        "            # If we can't find the word in glove, randomly initialize\n",
        "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
        "\n",
        "    # This is our \"rare word\" id - we want to randomly initialize\n",
        "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
        "    emb/=3\n",
        "    return emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEpBWca8OZB"
      },
      "source": [
        "emb = create_emb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JK5p4Ye9F8C"
      },
      "source": [
        "emb.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeahbE729Gm-"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.4, weights=[emb]),\n",
        "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    MaxPooling1D(),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-fsiRdg9Qfq"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_irj52T9Stw"
      },
      "source": [
        "model.fit(trn, y_train, validation_data=(test, y_test), nb_epoch=4, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjVmyUU_9clZ"
      },
      "source": [
        "# Transfer learning my approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy91B3f6Q7-p"
      },
      "source": [
        "## Fine Tune language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_N1Tn7Th2Jx"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from scipy.spatial.distance import cosine as dist"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McEmpStkdMm5"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/wordemb_dataset.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkgWft0Dqn5X",
        "outputId": "9135a97b-0b0c-4090-be85-fadba63b02a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instance</th>\n",
              "      <th>prep</th>\n",
              "      <th>geo-feature</th>\n",
              "      <th>placename</th>\n",
              "      <th>sum</th>\n",
              "      <th>or</th>\n",
              "      <th>label exp1</th>\n",
              "      <th>label exp 2</th>\n",
              "      <th>label exp 3</th>\n",
              "      <th>label exp 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>any project searching for a shared female ide...</td>\n",
              "      <td>for</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>searching a shared female identity in the wor...</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>misleading yet the shared endeavour by women ...</td>\n",
              "      <td>by</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>shared endeavour women to succeed in their ch...</td>\n",
              "      <td>in</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>succeed their chosen field and on their own t...</td>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            instance  ... label exp 4\n",
              "0   any project searching for a shared female ide...  ...         0.0\n",
              "1   searching a shared female identity in the wor...  ...         0.0\n",
              "2   misleading yet the shared endeavour by women ...  ...         0.0\n",
              "3   shared endeavour women to succeed in their ch...  ...         0.0\n",
              "4   succeed their chosen field and on their own t...  ...         0.0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0M2xCaT0Fw8"
      },
      "source": [
        "# remove 'na' fields\n",
        "df = df[df['instance'].notna()]\n",
        "df = df[df['label exp 2'].notna()]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3zl8t6byO_8",
        "outputId": "205df6aa-a3a0-467c-c3fd-290220bdf7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# splitting into train test sets\n",
        "test_df=df[16934:].copy()\n",
        "train_df=df[:16934].copy()\n",
        "len(test_df),len(train_df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1882, 16934)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1HNwrW49UH8",
        "outputId": "a3289ff8-9dba-4138-947d-1112fa32cb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "from fastai.text import * \n",
        "# after tokenisation\n",
        "data_lm = (TextList.from_df(df, cols='instance')\n",
        "                .split_by_rand_pct(0.1)\n",
        "                .label_for_lm()  \n",
        "                .databunch(bs=48))\n",
        "data_lm.show_batch()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  idx_min = (t != self.pad_idx).nonzero().min()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>xxunk gives the following xxunk for each one xxbos may have come off worst in some scuffle over a doe xxbos come off worst some scuffle over a doe and taken it xxbos to the xxunk barbecue for sandra s birthday xxbos it was very xxunk of me xxbos filled wonder at the sound of the bazouki xxbos filled wonder the sound of the bazouki as jacques vassal xxbos wonder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>th and which with all the houses buildings barns xxbos shillings a year and that another part was a new xxbos new xxunk or house built at southwell worth shillings a xxbos xxunk to the manor was worth shillings a year and xxbos am take off was followed by an xxunk flight and arrival xxbos an xxunk flight and arrival from a northerly direction at newbury xxbos and arrival a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>day in xxbos pyramids giza xxunk their day in a habit not unlike the xxbos their day a habit not unlike the great british tea break xxbos a second later he was in the doorway glancing from one xxbos he was the doorway glancing from one to the other in xxbos glancing one to the other in concern xxbos a series of studies by the research group xxbos a series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>case trevor lee advertisement manager with the northern echo said xxbos said we were asked by officers of darlington trading standards xxbos we were asked officers of darlington trading standards to produce xxbos trading standards to produce evidence of a mistake or otherwise xxbos after the expulsion of the jesuits xxbos the expulsion of the jesuits from portugal in xxbos the expulsion the jesuits from portugal in the building xxbos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>is in xxbos death his own life is in xxunk xxbos and xxunk knew better than to argue with him when xxbos xxunk knew better to argue with him when he used that xxbos argue him when he used that particular tone of voice xxbos when he used particular tone of voice xxbos she had wanted to xxunk at him and who was to xxbos and who was to blame</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB0_ofWA-Ive",
        "outputId": "09f865e8-e46e-4743-db42-3570f53e842e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5).to_fp16()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsH-k1OodxL3"
      },
      "source": [
        "drop_mult , a hyper-parameter ,used for regularization, sets the amount of dropout. If the model is over-fitting increase it, if under-fitting, you can decrease the number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1FAY5wYAyy5",
        "outputId": "14f05197-8a4c-4372-e2f6-6d214f5d18c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      50.00% [1/2 00:25<00:25]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.953056</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='45' class='' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      83.33% [45/54 00:21<00:04 11.8272]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9UQTOjA1f_",
        "outputId": "ba1ab016-27c4-476f-d740-67c89ad49b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqKR-9eRd7Kt"
      },
      "source": [
        "Learning rate hyper-parameter is one of the most important parameters to train a model. Fast.ai provides a convenient utility (learn.lr_find) to search through a range of learning rates to find the optimum one for our dataset. Learning rate finder will increase the learning rate after each mini-batch. Eventually, the learning rate is too high that loss will get worse. Now look at the plot of learning rate against loss and determine the lowest point (around 1e-1 for the plot below) and go back by one magnitude and choose that as a learning rate (something around 1e-2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bECNiwp2A3l0"
      },
      "source": [
        "#define batch size and learning rate\n",
        "bs=48\n",
        "lr = 2e-02\n",
        "lr *= bs/20"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgO1tRGvA-_1",
        "outputId": "a7b09ca6-7fd2-4b4e-f338-ee10dbd6d6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# fit the classifier for one cycle\n",
        "learn.fit_one_cycle(1, lr, moms=(0.8,0.7))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.580949</td>\n",
              "      <td>4.972424</td>\n",
              "      <td>0.196875</td>\n",
              "      <td>00:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddgTb3tiBCN3",
        "outputId": "3e78a0a2-ba84-4ced-c7d1-188fd65ae307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# unfreeze all layers and then train some more\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, lr/10, moms=(0.8,0.7))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.667150</td>\n",
              "      <td>4.642206</td>\n",
              "      <td>0.231399</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sj4G9jsBQqt"
      },
      "source": [
        "# save the encoder and vocab\n",
        "learn.save('fine_tuned_10')\n",
        "learn.save_encoder('fine_tuned_enc_10')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWcweo5QQ1R2"
      },
      "source": [
        "## Classification Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsSumeM6BuaU",
        "outputId": "e7be47ef-1670-4597-e8c5-4bbc60594b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# define dataset for classification \n",
        "data_clas = (TextList.from_df(train_df, cols=['instance'], vocab=data_lm.vocab)\n",
        "             .split_by_rand_pct(0.1)\n",
        "             .label_from_df(cols= 'label exp 2')\n",
        "             .databunch(bs=48))\n",
        "\n",
        "data_clas.show_batch()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos &amp; c which has been washed down and in which xxunk timber</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos i am writing to you about the case of i m xxunk</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos the lad darted the room like a fleeing hare and vanished</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxunk which xxunk had xxunk from his own soul though he</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos and his wife xxunk daughter of thomas xxunk of xxunk lincolnshire</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fz1Lf0XGKgu"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "@np_func\n",
        "def f1(inp,targ): return f1_score(targ, np.argmax(inp, axis=-1))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfgHr9FLB7-Q"
      },
      "source": [
        "#initialising classifier\n",
        "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5,metrics=[accuracy]).to_fp16()\n",
        "learn_c.load_encoder('fine_tuned_enc_10')\n",
        "learn_c.freeze()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewe8ZEoFCMnx",
        "outputId": "3521de2b-e1ed-437e-9f75-d7dfd231fa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "learn_c.lr_find()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='86' class='' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      27.13% [86/317 00:03<00:09 0.6302]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uae5qCUqCY4B",
        "outputId": "fcbafb3b-9174-41fa-b920-55596b9a0121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_c.recorder.plot(skip_end=15)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEGCAYAAABGnrPVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZd7G8e8vPUAIgdBDJzTpBFRsiIroumKDBbtrb6yivuquu7q6q766q7sWVFzRtSKKhbWuCqIgIAFCL4YeQAi9hdTn/WNG3xECJGFOziS5P9c1F3PazO9hBu55TnmOOecQEREJlyi/CxARkepFwSIiImGlYBERkbBSsIiISFgpWEREJKxi/C4gXFJTU13r1q39LkNEpEqZPXv2Fudcw3C+ZrUJltatW5OZmel3GSIiVYqZrQn3a2pXmIiIhJWCRUREwkrBIiIiYaVgERGRsFKwiIhIWHkaLGY22MyWmVm2md1TyvKWZjbZzOaa2XwzOzs4v7WZ5ZlZVvDxvJd1iohI+Hh2urGZRQPPAmcAOcAsM5vonFscstp9wHjn3HNm1gX4BGgdXLbCOdfTq/pERMQbXvZY+gHZzrmVzrkCYBww5IB1HFA3+DwZ2OBhPaXasa+Af3y5nKU/7qrstxYRqZa8DJbmwLqQ6ZzgvFAPAJeaWQ6B3sqtIcvaBHeRTTGzk0p7AzO7zswyzSwzNze3QkUaxujJK3gnM6dC24uIyC/5ffB+BPCKcy4NOBt4zcyigI1AS+dcL2AU8KaZ1T1wY+fcGOdchnMuo2HDio1IkFwrllM7NeTDrA0UFZdUvCUiIgJ4GyzrgRYh02nBeaGuBsYDOOemAwlAqnMu3zm3NTh/NrAC6OBVoef3SmPLnnymZm/x6i1ERGoML4NlFpBuZm3MLA4YDkw8YJ21wGkAZtaZQLDkmlnD4MF/zKwtkA6s9KrQUzs1JDkxlg/mHph7IiJSXp4Fi3OuCLgF+BxYQuDsr0Vm9qCZnRtc7Q7gWjObB7wFXOmcc8DJwHwzywLeBW5wzm3zqtb4mGh+1b0pny/axN78Iq/eRkSkRrDA/+NVX0ZGhjua0Y0zV2/jouen8/ehPbiwT1oYKxMRiVxmNts5lxHO1/T74H3E6NMqhRb1E/kgS7vDRESOhoIlyMw4v2dzpmVvYdOu/X6XIyJSZSlYQpzXqzklDj4spddSVFzCnLXbqS67DkVEvKJgCdG2YR16tKjH+3N/OQBAXkEx1782mwtGf8eEOdpVJiJyOAqWA5zfsxlLNu76eYiXbXsLuPhfM5i8bDONkuJ56qsfKNSFlCIih6RgOcCvezQjOsp4f8561m3bx0XPf8eiDbsYfUkfHr2wG2u37ePd2Rr+RUTkUDwb3biqalAnnlM6NGTCnBzen7ue/YXFvHHNsfRtXR/nHD1b1OOZSdlc0Ls58THRfpcrIhJx1GMpxfm9mrNlTwHRUca7N/anb+v6QODMsVFndGD9jjzGz1p3hFcREamZFCylGNy1CX86pwvv3dSfDo2TfrHspPRU+rZO4ZnJ2ewvLPapQhGRyKVgKUVsdBS/PbENTZMTD1pmZtx+Rgc27crnzZlrfahORCSyKVgqoH+7VI5v24DRX68gr0C9FhGRUAqWCho1qANb9uTz2ozVfpciIhJRFCwV1Ld1fU5KT+X5KSvZmVfodzkiIhFDwXIU7h7ciZ15hdz/4UK/SxERiRgKlqPQtXkyIwem80HWhlLHFxMRqYkULEfp5lPb0atlPe77YCHrd+T5XY6IiO8ULEcpJjqKf/ymJ8UljjvGZ1FSotGPRaRmU7CEQasGtXng18cwY+U2/jV1pd/liIj4SsESJkMz0hh8TBMe/3wZizfs8rscERHfKFjCxMx4+IJupNSK47a352pofRGpsRQsYVS/dhx/Pb8byzftYXymBqkUkZpJwRJmp3duRJ9WKTz9lQapFJGaScESZmbGnYM68uOu/bw+Y43f5YiIVDoFiweOb9eAk9JTGf31CvbkF/ldjohIpVKweOSOQR3ZtreAl6eu8rsUEZFKpWDxSM8W9TijS2PGfLOSHfsK/C5HRKTSKFg8dMegDuwpKOKFb3TRpIjUHAoWD3VqUpdfd2/GK9NWs3n3fr/LERGpFAoWj91+RgcKiksYPXmF36WIiFQKBYvH2qTWZlhGGv+evpox36zAOQ1SKSLVm6fBYmaDzWyZmWWb2T2lLG9pZpPNbK6ZzTezs0OW3RvcbpmZnellnV67/9fHcHbXpjz8yVLumbCAgiIN9yIi1VeMVy9sZtHAs8AZQA4wy8wmOucWh6x2HzDeOfecmXUBPgFaB58PB44BmgFfmlkH51yVvJQ9ITaap0f0om3D2jw9KZu12/bx3KW9qVcrzu/SRETCzsseSz8g2zm30jlXAIwDhhywjgPqBp8nAxuCz4cA45xz+c65VUB28PWqrKgo445BHXliWA9mr9nOBaO/Y9WWvX6XFVFyd+eTV1AlfzuISAgvg6U5EDoSY05wXqgHgEvNLIdAb+XWcmyLmV1nZplmlpmbmxuuuj11Qe803rj2WHbkFXL+6GnMXbvd75IiwtY9+Zz+xBTOefpbNuhOnCJVmt8H70cArzjn0oCzgdfMrMw1OefGOOcynHMZDRs29KzIcOvbuj7v39SfugmxXPziTKYsrxqh6KXHP1/G3vwiNu/K56LnvmNF7h6/SxKRCvIyWNYDLUKm04LzQl0NjAdwzk0HEoDUMm5bpbVqUJt3bzye1qm1ufqVWXyYVa2aVy7zc3bwduY6ruzfmreuO46C4hKGPj+dBTk7/S5NRCrAy2CZBaSbWRsziyNwMH7iAeusBU4DMLPOBIIlN7jecDOLN7M2QDrwvYe1+qJRUgJvX38cvVul8LtxWbw8reaNK1ZS4nhg4iIa1I5n5OnpdG2ezDs39CcxNprhY6bz3YotfpcoIuXk2VlhzrkiM7sF+ByIBsY65xaZ2YNApnNuInAH8KKZ3U7gQP6VLnChxyIzGw8sBoqAm6vqGWFHUjchlld/24+Rb83lz/9ZTEFRCdef0s7vssJq4rwNbNiRxzUntiEm+pe/Zd6fu545a3fw+EXdqZsQCwSu/ZlwY38uHzuTK8fO4rh2DbDg+mYQExXFlf1bc2J6aiW3RETKwqrLBXsZGRkuMzPT7zIqrKi4hGtezSRr3Q5m33cG0VF25I2qgJ15hZzw6CT25BfRv10DnhrRi9Q68QDs3l/IwL9PoXm9RN67sT9RB7R5x74C7vtgIeu2Bw/mB7+rm3bls2n3fu4c1JEbT2l30HYAa7bu5YO5G6hXK5YW9RNpkVKLtJRaJMZFe9tgkSrGzGY75zLC+Zqe9VikfGKioxjapwVfL8tl7trtZLSu73dJYfHa9NXsyS/illPb8+K3Kznnqak8e0lv+rRK4ZlJ2eTuzudfl2eUGg71asXxzMW9D5q/r6CIeyYs4PHPlzF37Q7+PqwHyYmB3s7WPfk8PSmbN2auobD44B9NzZIT+FX3pgzNaEGHxklhb6+IKFgiykkdUomJMr5aurlaBEteQTFjp61mQMeG3HlmR87q1oQbX5/Db16YzvWntGXstFUMy0ijR4t65XrdWnEx/HN4T3q1rMdfP17CkGem8uRvevLdiq089/UK9hUU8Zu+Lbnt9HTMYN22PHK272Pdtn3My9nJy9NW8+K3q+iRlsxFGS04p1tT6tWKxax69BJF/KZdYRFmxJgZbNtbwOe3n+x3KUft39+t5v6Jixh//fH0axMIyp37Chk1Pouvlm4mKT6GSXcOoGFSfIXfY9bqbdz0xhxyd+cDcEaXxtw9uCPtGx26N7JlTz4fzF3Pu7NzWPrjbgDioqOomxhLcmIMyYmx9GqZwh/O7lxqT0qkOtGusBrgtM6N+MvHS8jZvo+0lFp+l1NhhcUljPlmJRmtUn4OFYDkWrG8eHkGb36/luYpiUcVKhC4JujjW0/khW9WcuYxTX7xXoeSWieea05qy9UntmHB+p1My97KjrwCduUVsjOvkNzd+bw0dRVN6iZw7cltj6o+kZpIwRJhBnYKBMvkpZu57PjWfpdTYROzNrB+Rx4PDjnmoGVRUcalx7UK23s1qpvAH8/pUu7tzIzuafXonvbLXXHOOa57bTaPf76Mkzs0pGMTHYsRKQ+/r7yXA7RtWIfWDWrx1dLNfpdSYSUljuemrKBTkyQGdmrkdznlZmY8ckE3khJiuP3tLI1GLVJOCpYINLBTY75bsZV9BUV+l1IhXy7ZRPbmPdw4oF2VPSCeWieeRy7oxuKNu3jqqx8OuV5JiWPdtn18tWQTo7/OZtTbWUyct+GQ64vUBNoVFoFO69yIsdNW8V32Vk7v0tjvcsrFOcfor1fQsn4tftWtqd/lHJVBxzThoj5pjP46m4GdG9G7ZQoQaGPmmu2M+WYl07K3sC9kROak+Bg+yFpPau04+rfXBZxSMylYIlDf1vWpEx/DV0s3V7lgmb5yK1nrdvCX87oedJV9VXT/r7swfcVWRr2dxUcjT2Ja9hZemLKCOWt3kFIrlov6pNG5aV06NK5DeuMkos0Y8uw0bn1rLh+NPJGmyYl+N0Gk0ilYIlBcTBQnpacyeelmnHNVZnfSitw93PXOfBolxXNRnzS/ywmLpIRY/ja0Bxf/awbHP/wVu/OLaFE/kQeHHMPQPi1KvZL/+Ut7M+SZadz0xhzevu544mKqfsCKlIe+8RFqYKdG/LhrP4s37vK7lDKZn7ODoc9PJ7+omLFX9iUhtvoMnXJ8uwbcfnoHOjRJ4ukRvZh8xwAuP771IYeHad8oiccu6sHctTt4+JMllVytiP/UY4lQAzoGzqaatGQzxzRLrvT3zysoLvO4WtOyt3Ddq5mk1I7jtauPpU1qbY+rq3wjT0tn5GnpZV7/V92bMmdtG16auopeLesxpOdB96kTqbbUY4lQDZPi6dGiHpOWVf5px2/MXEPXBz7nH18up7jk8CMzfLJgI1e9PIu0lFpMuLF/tQyVirrnrE70bZ3CPRMW8NnCH/lx536qy0gXIoejHksEO61TI578cjlb9uT/PCJwUXEJizfuCtwfvrCYvIJi9hcWU1DsOKNzY1o2OLqr9TfsyOPhj5dQLzGWf3z5A9NXbOWfw3vRJDnhF+tt3rWfl6auYsy3K+ndMoWXrsigXq24o3rv6iY2OopnL+7Nuc9M44bXZwNQr1YsHRon0alJEsMyWtC1eeX3RkW8prHCItjC9Ts55+mp3HJqe2rFRzNz5TYyV29jb0Hpt6ZJio/h8aE9GNy1SYXezznH1f/OZPqKrfz39pP5ftU2/vjhQuJjovj7sB4M7NSYFbl7ePGblbw3Zz1FJSUM6dmcv57flVpx+o1yKHvzi1iwfifLN+1m6Y+7WfbjbpZu3EVeYTHXntyW20/vUK2OSUnV4sVYYQqWCOac47hHvmLTrsAAi+mN6nBs2/oc26YBrRrUIjE2moTgY/f+Qm5/O4t5OTu5/uS23HVmx3Kf7jtx3gZGvjWXP57ThatPbAMEzvS65c25LNm4i14t65G1bgdx0VEMzUjjmhPb0lq7vipkZ14hj366hLe+X0frBrV49MLuHNe2gd9lSQ2kYDmM6hgsAFnrdrBxRx792tSnQZ3DD9iYX1TMQx8t5vUZa+nXpj7PjOhFo7oJFBaXsGrLXpb+uJsNO/I4p3vTgwa43L63gNOfmEJaSiLv3XTCL240tr+wmEc/XcrkZZsZ0rM5lx/f6uddc3J0vsvewj3vLWDttn1cfGxL7vtVZ/X+pFIpWA6jugZLRbw3J4ffv7+AOvExNKgdz8ote35x06vE2Gh+d3o6V5/Yhthgr2bU+CwmZm3gP7eeSOemdf0qvUbKKyjmiS+W8a+pq/jtCW0qNKCmSEVp2Hwpkwt6p9GlWV0e+mgx8THRDOzciI6Nk+jQOIlacdH89ZMlPPrpUt6fs56/nt+VvQXFvDdnPbcObK9Q8UFiXDR/+FUXtu8r5PUZa7ju5LY0rptw5A1FIpR6LDXUfxf9yAMTF7Fh536S4mNoWDeeT0aepIPIPlq7dR+n/v1rLjuuFQ+ce/DtBkS84EWPRdex1FCDjmnCF6NO4fqT2xITbTx2YXeFis9aNqjF0D5pvDlzLRt35vldjkiFKVhqsNrxMdx7dmfm/mkQGa2PfOdF8d4tA9vjcDw7OdvvUkQqTMEiEkHSUmrxm74teHvWOnK27/O7HJEKUbCIRJibT22PYTwzSb0WqZoULCIRpmlyIhcf25J3ZuewZutev8sRKTcFi0gEunFAO2KijKcnZeOcY/2OPD6ev5G/fryYm96YTe7ufL9LFDkkXcciEoEa103g0uNa8fK0VUxZnvtzkMTFRFFUXEKz5ETu04WUEqEULCIR6sYB7Vj64y4aJSXQs0U9eraoR+emdbl7wnzemLmWm05tT/3aGlFaIo+CRSRCpdaJ541rjjto/k0D2vH+3PW8Mm0VowZ19KEykcPTMRaRKia9cRJnHtOYV75bze79hX6XI3IQBYtIFXTzqe3Ztb+I12es9bsUkYN4GixmNtjMlplZtpndU8ryJ80sK/hYbmY7QpYVhyyb6GWdIlVN97R6nJSeyktTV7K/sPQbv4n4xbNgMbNo4FngLKALMMLMfnEai3PududcT+dcT+Bp4L2QxXk/LXPOnetVnSJV1S2ntmfLngLenrXO71JEfsHLHks/INs5t9I5VwCMA4YcZv0RwFse1iNSrfRrU5+MVim8MGUFBUUlfpcj8jMvg6U5EPpTKic47yBm1gpoA0wKmZ1gZplmNsPMzjvEdtcF18nMzc0NV90iVYKZcfPA9mzYuZ8Ps9aXe/uSEsdr01fz5BfLKSmpHrfPkMgQKacbDwfedc6F7ixu5Zxbb2ZtgUlmtsA5tyJ0I+fcGGAMBO7HUnnlikSGAR0ackyzujw9KZttewuIiY4iLtqIjY6ifu04BnRsRFzMwb8fN+7M48535jEteysAe/KLuO9XnTGzg9YVKS8vg2U90CJkOi04rzTDgZtDZzjn1gf/XGlmXwO9gBUHbypSc5kZo87owLWvZvLIp0sPWp5aJ56Lj23JJce2/PmulBPnbeC+9xdQVOJ45IJuLN+0m5emrqJ+7ThuPrV9ZTdBqiEvg2UWkG5mbQgEynDg4gNXMrNOQAowPWReCrDPOZdvZqnACcBjHtYqUmWd1rkxy/5yFoXFJRQWOQpLSigsLmHZj7t5dfoanp70A6MnZ3NWt6Y45/ho/kZ6tazHk8N60jq1NiUljh37Cnn882XUrx3HiH4t/W6SVHGeBYtzrsjMbgE+B6KBsc65RWb2IJDpnPvpFOLhwDj3y3skdwZeMLMSAseBHnXOLfaqVpGqLjY6itjoKAgZ4aVpciIDOjZi9Za9vDZjDeNnrSOvsJg7zugQGOQyOrCLLCrKeOyi7uzYV8Af3l9AvcRYzurW1KeWSHWge96L1BB784vYX1hMgzrxpS7PKyjm0pdmsiBnJy9c1odTOzWq5ArFD7rnvYhUWO34mEOGCkBiXDRjr+hL24a1ueqVWVz20kxmrd5WiRVKdaFgEZGfJdeKZcKN/bn3rE4s2biLoc9PZ8SYGUxfsZXqsndDvKddYSJSqryCYt78fi3PT1lB7u58hvRsxhPDehIdpVOSqxPtChORSpMYF83VJ7bh2/85lZGnpfNh1gb+/J9F6rnIEUXKBZIiEqESYqMZdUYH8guLeeGblTRKiueWgel+lyURTMEiImVy9+BObN6dz9/+u5xGSQkM69viyBtJjVSmYDGz2gRGGy4xsw5AJ+BT55zuMiRSQ/x0vcvWvQXc+/4CGtSJ47TOjf0uSyJQWY+xfENgUMjmwH+By4BXvCpKRCJTbHQUz13Sm2Oa1eXmN+fw0tRVTMvewsadeTr2Ij8r664wc87tM7OrgdHOucfMLMvLwkQkMtWOj2HslX255MWZPPTR/w+IUSsumrYNazOiX0su7tdSA1rWYGUOFjM7HrgEuDo4L9qbkkQk0qXWieez205i0658VubuYcWWvazM3cOctTv4w/sL+WLxJh67sDuNggNfSs1S1mC5DbgXeD843ldbYLJ3ZYlIpDMzmiQn0CQ5gf7tU4HgPV5mrOGRT5cw6B/f8NfzuvGr7hp3rKYp9wWSZhYF1HHO7fKmpIrRBZIikWNF7h5GvZ3FvJydnNezGQ+e15W6CbF+lyWl8O0CSTN708zqBs8OWwgsNrO7wlmIiFQf7RrW4d0b+3Pb6en8Z/5Gzn16Kks2RtRvUfFQWc8K6xLsoZwHfErgNsKXeVaViFR5sdFR3HZ6B8Zddxz7Coo5f/Q03p+b43dZUgnKGiyxZhZLIFgmBq9f0bmFInJEfVvX56ORJ9I9rR63vz2PP324kIKiEr/LEg+VNVheAFYDtYFvzKwVoH6tiJRJo6QE3rjmWK49qQ2vTl/Db8ZMZ9Ou/X6XJR6p8OjGZhbjnCsKcz0VpoP3IlXDJws2cuc786iXGMvYq/rSqUldv0uq0fw8eJ9sZk+YWWbw8XcCvRcRkXI5u1tTxl9/PMXOcdFz0/lmea7fJUmYlXVX2FhgNzAs+NgFvOxVUSJSvXVtnswHN59AWkoiV70yi3Hfr/W7JAmjsgZLO+fc/c65lcHHn4G2XhYmItVb0+RE3rnheE5on8o97y3gsc+WUlKic4Kqg7IGS56ZnfjThJmdAOR5U5KI1BRJCbG8dEUGI/q1ZPTXK3jo48UazLIaKOuQLjcAr5pZcnB6O3CFNyWJSE0SGx3Fw+d3JTE2mrHTVpEUH8OoQR39LkuOQpmCxTk3D+hhZnWD07vM7DZgvpfFiUjNYGb88ZzO7M0v4qlJ2dSOj+H6U9r5XZZUULnuIHnA+GCjgH+EtxwRqanMjIcv6MaegiIe+XQpdRJiuOTYVn6XJRVwNLcm1s0WRCSsoqOMJ4f1JK+gmPs+WEituGjO75Xmd1lSTmU9eF8aHWETkbCLi4li9CW9Oa5NA+58Zz6vz1jjd0lSTocNFjPbbWa7SnnsBppVUo0iUsMkxEbzrysyODk9lfs+WMhDHy2mWKciVxmHDRbnXJJzrm4pjyTn3NHsRhMROaza8TG8eHkGV/ZvzUtTV3H9a7PZmx8xo0jJYRzNrjAREU/FREfxwLnH8Odzj2HS0k0MfX46G3fqErpIp2ARkYh3Rf/WvHRlX9Zu28eQZ6Yxedlmv0uSw/A0WMxssJktM7NsM7unlOVPmllW8LHczHaELLvCzH4IPnQxpkgNd2rHRrx74/HUqxXLVS/P4q535rEzr9DvsqQUFR42/4gvbBYNLAfOAHKAWcAI59ziQ6x/K9DLOfdbM6sPZAIZBM4+mw30cc5tP9T7adh8kZohv6iYp776geenrKRhnXgeubAbp3Zs5HdZVZZvw+ZXUD8gOzhoZQEwDhhymPVHAG8Fn58JfOGc2xYMky+AwR7WKiJVRHxMNHed2Yn3b+pP3cQYrnp5FvdMmK+7UkYQL4OlObAuZDonOO8gwTtStgEmlWdbM7vup3vE5Obqng4iNUn3tHr859YTuXFAO8bNWsd1r2WSV1Dsd1lC5By8Hw6865wr17fCOTfGOZfhnMto2LChR6WJSKSKj4nm7sGdeOSCbkxZnssVL3/P7v067uI3L4NlPdAiZDotOK80w/n/3WDl3VZEargR/Vryz+G9mLNmOxe/OJNtewv8LqlG8zJYZgHpZtbGzOIIhMfEA1cys05ACjA9ZPbnwCAzSzGzFGBQcJ6ISKnO7dGMMZf3Yfmm3Qx7YTo/7tzvd0k1lmfB4pwrAm4hEAhLgPHOuUVm9qCZnRuy6nBgnAs5Pc05tw14iEA4zQIeDM4TETmkgZ0a88pV/di4I48hz05l0tJNfpdUI3l2unFl0+nGIvKThet3Mmp8Fss37eH8Xs350zldSKkd53dZEamqnW4sIuKLrs2T+c+tJzLytHT+M28DZzw5hU8WbPS7rBpDwSIi1VJ8TDSjzujAxFtOpElyAje9MYf/eXce1WUvTSRTsIhItdalWV0+uOkEbjilHeMzcxj99Qq/S6r2NPS9iFR7MdFR3D24Ixt35vG3/y6jY+MkTu/S2O+yqi31WESkRjAz/vfC7nRtlsxtb2fxw6bdfpdUbSlYRKTGSIiNZszlfUiIjeaaVzPZsU8XUnpBwSIiNUrT5EReuKw3G3fs59a35lJUrMErw03BIiI1Tp9W9fnLeV359oct/P79BRoZOcx08F5EaqRhfVuQs30fT03KZvWWfYy+tDepdeL9LqtaUI9FRGqsUYM68s/hPZmXs4NfPz2V+Tk7jryRHJGCRURqtCE9mzPhxv5EmXHR89OZMDvH75KqPAWLiNR4XZsnM/GWE+jdsh53vDOPJ/67zO+SqjQFi4gI0KBOPK9dfSzDMtJ4alI2o7/O9rukKksH70VEgmKjo3jkgu7kF5Xw2GfLqB0XwxX9W/tdVpWjYBERCREdZfxtaA/2FRRz/8RF1IqLZmhGiyNvKD/TrjARkQPERkfx9IhenNg+lbsnzOfj+RpyvzwULCIipfhp+JfeLVP43bi5TF662e+SqgwFi4jIIdSKi2HsVX3p1DSJG16fzcyVW/0uqUpQsIiIHEbdhFj+fVU/0lISuebfmSxcv9PvkiKegkVE5Aga1Inn9WuOpW5iLJeP/Z7szXv8LimiKVhERMqgaXIir19zLFFmXPbSTHK27/O7pIilYBERKaM2qbV57ep+7M0v4tJ/zSR3d77fJUUkBYuISDl0blqXl6/qx4+79nPnO/NwzvldUsRRsIiIlFOfVince1ZnpizP5c3v1/pdTsRRsIiIVMBlx7XipPRU/vLRElZv2et3ORFFwSIiUgFRUcZjF3UnJtq44515FJdol9hPFCwiIhXUNDmRh4Z0Zfaa7Yz5ZqXf5UQMBYuIyFEY0rMZZ3drwhNfLGPJxl1+lxMRFCwiIkfBzPjLed1ITozj9rezyC8q9rsk3ylYRESOUv3acTx2UTeW/rib16av8bsc33kaLGY22MyWmVm2md1ziHWGmdliM1tkZm+GzC82s6zgY6KXdYqIHK2BnRrTv10DXvhmJfsLa3avxbNgMbNo4HbmYlkAAA21SURBVFngLKALMMLMuhywTjpwL3CCc+4Y4LaQxXnOuZ7Bx7le1SkiEi4jT0snd3c+42r4tS1e9lj6AdnOuZXOuQJgHDDkgHWuBZ51zm0HcM7phgciUmUd17YB/drU57kpK2p0r8XLYGkOrAuZzgnOC9UB6GBm08xshpkNDlmWYGaZwfnnlfYGZnZdcJ3M3Nzc8FYvIlIBvzstnU278nlndo7fpfjG74P3MUA6MAAYAbxoZvWCy1o55zKAi4F/mFm7Azd2zo1xzmU45zIaNmxYWTWLiBxS/3YN6NMqhecmZ1NQVOJ3Ob7wMljWAy1CptOC80LlABOdc4XOuVXAcgJBg3NuffDPlcDXQC8PaxURCQszY+Rp6WzYuZ8Jc2pmr8XLYJkFpJtZGzOLA4YDB57d9QGB3gpmlkpg19hKM0sxs/iQ+ScAiz2sVUQkbE5OT6VHi3qM/jqbwuKa12vxLFicc0XALcDnwBJgvHNukZk9aGY/neX1ObDVzBYDk4G7nHNbgc5AppnNC85/1DmnYBGRKsHM+N1p7Vm3LY8P5h64o6b6s+pyL4GMjAyXmZnpdxkiIgA45/j1M1PZs7+IL0edQky034e0S2dms4PHs8MmMlsqIlLFmRkjB6azeus+Psja4Hc5lUrBIiLikTO6NKZr87r886vlNepYi4JFRMQjZsYdZ3Rk3bY83smsOWeIKVhERDw0oGNDeresx9OTfqgxV+MrWEREPGRm3DmoIxt37uetGjKGmIJFRMRj/duncnzbBjw7eQV5BdW/16JgERGpBHcM6sCWPfm8On2136V4TsEiIlIJMlrX55QODXl+ygp27y/0uxxPKVhERCrJHYM6sH1fIS9PW+13KZ5SsIiIVJLuafUY1KUxL36zki178v0uxzMKFhGRSvQ/gzuSX1zC3e/Op7oMqXUgBYuISCVq3yiJe8/qxFdLN/P6jDV+l+MJBYuISCW7sn9rTunQkL98vIQfNu32u5ywU7CIiFQyM+Pxod2pEx/DyHFZ5BdVr2tbFCwiIj5olJTAYxd1Z8nGXfzt82V+lxNWChYREZ+c1rkxlx3Xihe/XcXUH7b4XU7YKFhERHz0+7M7075RHUaNz2JrNTkFWcEiIuKjxLhonhreix37CrmrmpyCrGAREfFZl2Z1+f3ZnZi0dDNjq8FV+QoWEZEIcEX/1pzeuTGPfrqEBTk7/S7nqChYREQigJnx+EXdaVA7nlvfmsOe/CK/S6owBYuISIRIqR3HP4f3ZO22ffzpg4V+l1NhChYRkQhybNsG3DownffmrmfC7By/y6kQBYuISIS5dWB7+rWpz58+XMi6bfv8LqfcFCwiIhEmJjqKJ4b1AODuCfMpKalapyArWEREIlBaSi3uO6cL363Yyhszq9YoyAoWEZEINbxvC05KT+XhT5aydmvV2SWmYBERiVBmxv9e2J2YKOOud+dVmV1iChYRkQjWrF4ifzynCzNXbePV6av9LqdMFCwiIhFuaEYaAzo25NHPlrJ6y16/yzkiT4PFzAab2TIzyzazew6xzjAzW2xmi8zszZD5V5jZD8HHFV7WKSISycyMRy/oTmx0FCPHzY34UZA9CxYziwaeBc4CugAjzKzLAeukA/cCJzjnjgFuC86vD9wPHAv0A+43sxSvahURiXRNkhN4clhPlv24m3OfmcaSjbv8LumQvOyx9AOynXMrnXMFwDhgyAHrXAs865zbDuCc2xycfybwhXNuW3DZF8BgD2sVEYl4p3dpzDs3HE9xiePC577js4Ub/S6pVF4GS3NgXch0TnBeqA5ABzObZmYzzGxwObYVEalxuqfVY+ItJ9ChcRI3vD6Hf375Q8Tdw8Xvg/cxQDowABgBvGhm9cq6sZldZ2aZZpaZm5vrUYkiIpGlUd0Exl13HBf0bs6TXy7nljfnRtSpyDEevvZ6oEXIdFpwXqgcYKZzrhBYZWbLCQTNegJhE7rt1we+gXNuDDAGICMjI3L+VkVEPJYQG83fh/agc5O67NpfSFSU+V3Sz7wMlllAupm1IRAUw4GLD1jnAwI9lZfNLJXArrGVwArg4ZAD9oMIHOQXEZEgM+Pak9v6XcZBPAsW51yRmd0CfA5EA2Odc4vM7EEg0zk3MbhskJktBoqBu5xzWwHM7CEC4QTwoHNum1e1iohI+FikHfSpqIyMDJeZmel3GSIiVYqZzXbOZYTzNf0+eC8iItWMgkVERMJKwSIiImGlYBERkbBSsIiISFgpWEREJKyqzenGZpYLhN4YOhnYecBqZZl3uOnQ56nAlqMo+XA1VXTdQy0vb7u9bvOhaqrIuuFq84HT1eWzPtLfQyR91n58v0Ona+r3u6NzLqmMtZaNc65aPoAxFZl3uOkDnmd6VWdF1z3U8vK22+s2h7Pd4Wpzdf2sj/T3EEmftR/f79Bpfb/D96jOu8L+U8F5h5subfujVZ7XPNK6h1pe3nZ73ebyvu7h1g1Xmw+cri6f9ZH+HiLps/bj+12W962Imvj9/lm12RVW2cws04X5atVIVxPbDDWz3WpzzeFFu6tzj8VrY/wuwAc1sc1QM9utNtccYW+3eiwiIhJW6rGIiEhYKVhERCSsFCyAmY01s81mtrAC2/YxswVmlm1mT5mZhSy71cyWmtkiM3ssvFUfHS/abGYPmNl6M8sKPs4Of+UV59XnHFx+h5m54A3rIopHn/VDZjY/+Dn/18yahb/yivOozY8H/z3PN7P3y3Mb9criUbuHBv8PKzGzsh3kD/f5y1XxAZwM9AYWVmDb74HjAAM+Bc4Kzj8V+BKID0438rudldDmB4A7/W5bZbY5uKwFgZvWrQFS/W5nJX3WdUPWGQk873c7K6HNg4CY4PP/Bf7X73ZWUrs7Ax0J3B4+oyyvpR4L4Jz7BvjFHSrNrJ2ZfWZms83sWzPrdOB2ZtaUwD+wGS7wCbwKnBdcfCPwqHMuP/gem71tRfl41OaI5mGbnwT+B4jIM2G8aLdzblfIqrWJsLZ71Ob/OueKgqvOANK8bUX5edTuJc65ZeWpQ8FyaGOAW51zfYA7gdGlrNMcyAmZzgnOA+gAnGRmM81sipn19bTa8DjaNgPcEtxVMNbMUrwrNWyOqs1mNgRY75yb53WhYXbUn7WZ/dXM1gGXAH/ysNZwCcf3+ye/JfCrvioIZ7vLxLN73ldlZlYH6A+8E7IrPb6cLxMD1CfQtewLjDeztsFfAxEnTG1+DniIwK/Xh4C/E/gHGJGOts1mVgv4PYFdJFVGmD5rnHN/AP5gZvcCtwD3h63IMAtXm4Ov9QegCHgjPNV5J5ztLg8FS+migB3OuZ6hM80sGpgdnJxI4D/S0O5wGrA++DwHeC8YJN+bWQmBQe5yvSz8KBx1m51zm0K2exH4yMuCw+Bo29wOaAPMC/6jTQPmmFk/59yPHtd+NMLx/Q71BvAJERwshKnNZnYlcA5wWqT+SDxAuD/rsvH7YFOkPIDWhBzwAr4DhgafG9DjENsdeMDr7OD8G4AHg887AOsIXpAaKQ8P2tw0ZJ3bgXF+t9HrNh+wzmoi8OC9R591esg6twLv+t3GSmjzYGAx0NDvtlVmu0OWf00ZD977/pcQCQ/gLWAjUEigp3E1gV+inwHzgl+mPx1i2wxgIbACeOan8ADigNeDy+YAA/1uZyW0+TVgATCfwK+gppXVHr/afMA6ERksHn3WE4Lz5xMY0LC53+2shDZnE/iBmBV8RNSZcB62+/zga+UDm4DPj1SHhnQREZGw0llhIiISVgoWEREJKwWLiIiElYJFRETCSsEiIiJhpWCRas3M9lTy+30XptcZYGY7g6MHLzWzv5Vhm/PMrEs43l/kaChYRMrBzA47WoVzrn8Y3+5bF7hiuhdwjpmdcIT1zwMULOI7BYvUOIca7dXMfh0cNHSumX1pZo2D8x8ws9fMbBrwWnB6rJl9bWYrzWxkyGvvCf45ILj83WCP442Q+1ucHZw3O3jfi8MOfeOcyyNwQd5PA19ea2azzGyemU0ws1pm1h84F3g82MtpV5ZRbUW8oGCRmuhQo71OBY5zzvUCxhEYCv8nXYDTnXMjgtOdgDOBfsD9ZhZbyvv0Am4LbtsWOMHMEoAXCNzrog/Q8EjFBkeJTge+Cc56zznX1znXA1gCXO2c+47AaAd3Oed6OudWHKadIp7SIJRSoxxhtNc04O3gvSnigFUhm04M9hx+8rEL3Gsn38w2A4355bDjAN8753KC75tFYAynPcBK59xPr/0WcN0hyj3JzOYRCJV/uP8f2LKrmf0FqAfUIXCTsfK0U8RTChapaUod7TXoaeAJ59xEMxtA4I6YP9l7wLr5Ic+LKf3fUlnWOZxvnXPnmFkbYIaZjXfOZQGvAOc55+YFR9sdUMq2h2uniKe0K0xqFBe48+EqMxsKYAE9gouT+f+hwq/wqIRlQFszax2c/s2RNgj2bh4F7g7OSgI2Bne/XRKy6u7gsiO1U8RTChap7mqZWU7IYxSB/4yvDu5mWgQMCa77AIFdR7OBLV4UE9yddhPwWfB9dgM7y7Dp88DJwUD6IzATmAYsDVlnHHBX8OSDdhy6nSKe0ujGIpXMzOo45/YEzxJ7FvjBOfek33WJhIt6LCKV79rgwfxFBHa/veBzPSJhpR6LiIiElXosIiISVgoWEREJKwWLiIiElYJFRETCSsEiIiJh9X+qc0JwCRNa1QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaxKXbnoCbo3"
      },
      "source": [
        "lr=2e-2"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_wrbz7mCf07",
        "outputId": "8f99a3c4-0d65-4bb5-a249-20c7f43c8077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "learn_c.fit_one_cycle(3,lr, moms=(0.8,0.7))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.179184</td>\n",
              "      <td>0.139875</td>\n",
              "      <td>0.558771</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.161983</td>\n",
              "      <td>0.137049</td>\n",
              "      <td>0.558771</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.155700</td>\n",
              "      <td>0.132537</td>\n",
              "      <td>0.558771</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmVK-Klif7HX",
        "outputId": "593a200b-949b-47f5-e51f-34577ed6776a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_c.recorder.plot_losses()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvvWWf7IgKvZ",
        "outputId": "7048bf00-576b-410f-a5da-42ae029d0d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_c.recorder.plot_lr(show_moms=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwggjnrkrUXq",
        "outputId": "90c8de9e-0e1e-44c5-d117-da9d6dc13b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_c.unfreeze()\n",
        "learn_c.fit_one_cycle(1, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.142500</td>\n",
              "      <td>0.117450</td>\n",
              "      <td>0.558771</td>\n",
              "      <td>00:38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uLBMMSYXmOv"
      },
      "source": [
        "### Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbr02KPZLPC9"
      },
      "source": [
        "The loss curve seems going down smoothly (not in this case) and has not reached saturation point yet. fastai calculates the exponentially weighted moving average of the losses thus makes it easier to read these charts [by making the curve smoother] at the same time it might be a batch or two behind where they should be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDAPRB_zK2CU"
      },
      "source": [
        "When we call fit_one_cycle, we are actually passing in a maximum learning rate. The left side plot shows learning rate change vs the batches. The learning starts slow and it increases about half the time and then it decreases about half the time. As you get close to the final answer you need to anneal your learning rate to hone in on it. The motivation behind this is that during the middle of learning when learning rate is higher, the learning rate works as regularization method and keep network from over-fitting. This helps the network to avoid steep areas of loss and land better flatter minima. Please refer to this paper by Leslie smith which talks in great detail about neural network hyper-parameter tuning and you can find most of these ideas implemented in fastai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__-WMMpnci50"
      },
      "source": [
        "If the training loss is still higher than the validation loss then we are not over-fitting yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAkVdPon6608"
      },
      "source": [
        "Since get_preds gets predictions on all the data in the ds_type dataset, here the number of predictions will be equal to the number of data in the validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDDE4lhaLAXs"
      },
      "source": [
        "Discriminative learning rates: Applying different learning rate to layers as you go from layer to layer. When fitting a model you can pass a list of learning rates which will apply a different rate to each layer group. When working with a Learner on which you've called split, you can set hyper-parameters in four ways:\n",
        "\n",
        "* param = [val1, val2 ..., valn] (n = number of layer groups)\n",
        "* param = val\n",
        "* param = slice(start,end)\n",
        "* param = slice(end)\n",
        "\n",
        "If we chose to set it in way 1, we must specify a number of values exactly equal to the number of layer groups. If we chose to set it in way 2, the chosen value will be repeated for all layer groups. If you pass slice(start,end) then the first group's learning rate is start, the last is end, and the remaining are evenly spaced.\n",
        "If you pass just slice(end) then the last group's learning rate is end, and all the other groups are end/10. For instance (for our learner that has 3 layer groups):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbYJb9f8LJbW"
      },
      "source": [
        "The bottom of the slice and the top of the slice is the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. As you go from layer to layer, we decrease the learning rate. The lowest levels are given smaller learning rates so as not to disturb the weights much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnmaLQmusHeE"
      },
      "source": [
        "The first two elements of the tuple are, respectively, the predicted class and label. Label here is essentially an internal representation of each class, since class name is a string and cannot be used in computation. To check what each label corresponds to, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr5WryK8TmRt"
      },
      "source": [
        "y_true=list(test_df['label exp 2'])\n",
        "del test_df['label exp 2']"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiGBl3Uu5g32"
      },
      "source": [
        "y_pred=[]\n",
        "for i in range(len(test_df)):\n",
        "  p=learn_c.predict(test_df.iloc[i])\n",
        "  if (float((p[0]).data[0]))<0.5:\n",
        "    y_pred.append(0.0)\n",
        "  else:\n",
        "    y_pred.append(1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0iStbcWhr6",
        "outputId": "f5923a12-f4e6-4f09-a74c-605c0eeb643c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "print(sklearn.metrics.classification_report(y_true, y_pred))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.83      0.78       800\n",
            "         1.0       0.86      0.77      0.81      1082\n",
            "\n",
            "    accuracy                           0.80      1882\n",
            "   macro avg       0.79      0.80      0.79      1882\n",
            "weighted avg       0.80      0.80      0.80      1882\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqtxa8sj36UD"
      },
      "source": [
        "# Backward\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-wMjj7u3QKh",
        "outputId": "332233c4-00c9-4263-a9a2-7b564d9320e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from fastai.text import * \n",
        "\n",
        "data_lm_bwd = (TextList.from_df(df, cols='instance')\n",
        "                .split_by_rand_pct(0.1)\n",
        "                .label_for_lm()  \n",
        "                .databunch(bs=48,backwards=True))\n",
        "data_lm_bwd.show_batch()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>licensing about manufacturers talking is xxunk xxbos invention his licensing about manufacturers with talking is xxunk xxbos plans new the into xxunk burning himself threw and xxbos shed xxunk xxunk the beside bar snack the onions frying xxbos the beside bar snack the from onions frying smell occasional an xxbos snack the from onions frying of smell occasional an pier the xxbos smell occasional an pier the down and pavements</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>xxunk he car the in herself locked actress xxunk the xxbos muddy be may times other at and xxunk be may footpaths xxbos xxunk be may footpaths the of some high is river the xxbos xxunk of force superior xxunk an attacked xxbos of force superior xxunk an by attacked suddenly is it xxunk xxbos attacked suddenly is it xxunk of pass the entering is xxunk xxbos engine northern great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>a edges mountain the down and far stretching terraces xxbos edges mountain the down and up far stretching terraces rice xxbos terraces rice famous the of sight first our xxunk we xxbos through bar tommy a xxunk of system rival the improvement an xxbos xxunk of system rival the over improvement an was this felt xxbos door fire the via alley back a xxunk we xxbos the via alley back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>allowing grounds and garden acre xxbos them of ahead close day pay and xxbos day pay and london and alongside right england old bound xxunk xxbos xxunk xxunk everybody channel the up well laying ship the and xxbos wind the xxunk port the on torbay was auckland city the xxbos xxunk port the on torbay in was auckland city the later xxbos on torbay in was auckland of city the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>source the is it clear xxbos the is it clear crystal so lake ringed hill a lake xxbos hill a lake otsego placid of tip southern the time stuck xxbos placid of tip southern the on time stuck proudly sits cooperstown xxbos tip southern the on time in stuck proudly sits cooperstown woodlands xxbos woodlands s york new upstate amid village lined tree gracious a xxbos new cooperstown york new</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-JzgACB3QUv",
        "outputId": "8091699a-9425-4d63-c878-c50b703f4806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_bwd = language_model_learner(data_lm_bwd, AWD_LSTM, drop_mult=0.5).to_fp16()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-bwd.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvgO_eUO3Qd2",
        "outputId": "84511c33-3ade-41da-8d2f-276599114f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "learn_bwd.lr_find()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='1' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      50.00% [1/2 00:25<00:25]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.965939</td>\n",
              "      <td>#na#</td>\n",
              "      <td>00:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='45' class='' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      83.33% [45/54 00:21<00:04 11.3510]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktw3Npff3Qbr",
        "outputId": "74472207-72f9-4bc2-e759-8307c25a192b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_bwd.recorder.plot()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6_Y5MeV3QZ8"
      },
      "source": [
        "bs=48\n",
        "lr_bwd = 2e-02\n",
        "# lr_bwd *= bs/20"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCo94ndraYm",
        "outputId": "6be6150a-011b-4260-fd0d-c212a92cb6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn_bwd.fit_one_cycle(1, lr_bwd, moms=(0.8,0.7))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.325920</td>\n",
              "      <td>4.629201</td>\n",
              "      <td>0.243204</td>\n",
              "      <td>00:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4FTBuEra49",
        "outputId": "ec80cfd6-b462-491b-e846-1f107f238ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_bwd.unfreeze()\n",
        "learn_bwd.fit_one_cycle(1, lr_bwd/10, moms=(0.8,0.7))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.746543</td>\n",
              "      <td>4.381660</td>\n",
              "      <td>0.282788</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYm1rNyFra0l"
      },
      "source": [
        "learn_bwd.save('fine_tuned_bwd')\n",
        "learn_bwd.save_encoder('fine_tuned_enc_bwd')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh91TZGcsN6n",
        "outputId": "04b2c3c6-c35c-4bf5-b323-9497afa46d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_df=df[16934:].copy()\n",
        "train_df=df[:16934].copy()\n",
        "len(test_df),len(train_df)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1882, 16934)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6-KXpuksN2V",
        "outputId": "fb24b37c-d3f3-4b74-de44-c6d6cc91f8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_clas_bwd = (TextList.from_df(train_df, cols=['instance'], vocab=data_lm_bwd.vocab)\n",
        "             .split_by_rand_pct(0.1)\n",
        "             .label_from_df(cols= 'label exp 2')\n",
        "             .databunch(bs=48,backwards=True))\n",
        "\n",
        "data_clas_bwd.show_batch()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>methods modern xxunk be not can that one and floor sound firm xxbos</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>about be might she what at afraid almost xxunk half question xxbos</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>street mount and row park between orchard large a was there xxbos</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>nice a and food good with pub country traditional a is xxbos</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>plants of diversity rich a for habitat important provides west the xxbos</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc1u3VQFtEy4"
      },
      "source": [
        "learn_c_bwd = text_classifier_learner(data_clas_bwd, AWD_LSTM, drop_mult=1.0,metrics=[accuracy]).to_fp16()\n",
        "learn_c_bwd.load_encoder('fine_tuned_enc_bwd')\n",
        "learn_c_bwd.freeze()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAb4lm5mtE-g",
        "outputId": "731a9024-ea4d-427c-8adb-33c8cf8f10c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "learn_c_bwd.lr_find()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='85' class='' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      26.81% [85/317 00:03<00:09 0.5395]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxFbQ0QtEuC",
        "outputId": "d013e271-d36d-4c26-c60b-21baafe7491c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn_c_bwd.recorder.plot()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6u_6ZuxcPW"
      },
      "source": [
        "bs=48\n",
        "lr_bwd = 1e-02\n",
        "lr_bwd *= bs/20"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHGt2rs5tEmY",
        "outputId": "4e8b43c2-f382-4936-f29b-85bb8f0ba962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn_c_bwd.fit_one_cycle(1,lr, moms=(0.8,0.7))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.199052</td>\n",
              "      <td>0.153910</td>\n",
              "      <td>0.561725</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFBdq6UTxkDP",
        "outputId": "42ee1af4-d047-4aa6-fc51-b5f8a5799cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_c_bwd.unfreeze()\n",
        "learn_c_bwd.fit_one_cycle(1, slice(lr_bwd/2,lr_bwd), moms=(0.5,0.7))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.162550</td>\n",
              "      <td>0.143528</td>\n",
              "      <td>0.561725</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:1319: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if p.grad is not None:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z8ap_3myiji"
      },
      "source": [
        "learn_c_bwd.save('learner_bwd')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9zYuVwQyi-Q",
        "outputId": "fe0cdae9-c86e-431b-b2ec-25c54969bad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds,targs = learn_c.get_preds(ordered=True)\n",
        "accuracy(preds,targs)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5588)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijSZxE_FzTJw",
        "outputId": "7959cc4e-8ad2-45a7-ca93-7243417d5913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_b,targs_b = learn_c_bwd.get_preds(ordered=True)\n",
        "accuracy(preds_b,targs_b)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5617)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4aUX3EdzTap"
      },
      "source": [
        "preds_avg = (preds+preds_b)/2"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DIuCDOdzsen",
        "outputId": "681bc5cf-73d2-423c-d5b7-13da5e117dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# average accuracy \n",
        "accuracy(preds_avg,targs_b)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5617)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaXwktduacnt"
      },
      "source": [
        "y_true=test_df['label exp 2']"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKwbCUoYzsob"
      },
      "source": [
        "forward_prob=[]\n",
        "for i in range(len(test_df)):\n",
        "  p=learn_c.predict(test_df.iloc[i])\n",
        "  forward_prob.append(float((p[0]).data[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3DFgA90RmF"
      },
      "source": [
        "bwd_prob=[]\n",
        "for i in range(len(test_df)):\n",
        "  p=learn_c_bwd.predict(test_df.iloc[i])\n",
        "  bwd_prob.append(float((p[0]).data[0]))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxX0vXUl0xV7"
      },
      "source": [
        "\n",
        "y_pred=[]\n",
        "for i in range(len(my)):\n",
        "  if (float((forward_prob[i]+bwd_prob[i])/2))>0.5:\n",
        "    y_pred.append(1.0)\n",
        "  else:\n",
        "    y_pred.append(0.0)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ss0NyNyN0_3"
      },
      "source": [
        "final_my= np.nan_to_num(final_my, copy=True)\n",
        "pred= np.nan_to_num(pred, copy=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2IFNrsIpzzY",
        "outputId": "d6a55181-eef7-484a-cbfd-af8d9cfb19c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.metrics.classification_report(y_true,y_pred))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.81      0.77       800\n",
            "         1.0       0.85      0.77      0.81      1082\n",
            "\n",
            "    accuracy                           0.79      1882\n",
            "   macro avg       0.79      0.79      0.79      1882\n",
            "weighted avg       0.79      0.79      0.79      1882\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}